{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a3ea993-4d21-4e76-809e-f09b82b2c0bf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#connection to databricks from adls\n",
    "spark.conf.set(\"fs.azure.account.key.<storage_account>.dfs.core.windows.net\",dbutils.secrets.get(scope=\"<secretscope>\", key=\"<accesskey>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9881ef9e-8f88-4cf8-9744-406cf572cb3e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#reading the Json file from path\n",
    "json_df = spark.read.json(f\"abfss://<container>@<storage-account>.dfs.core.windows.net/<file-path>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed3ca14c-daba-44ae-97de-20063e45eaef",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode, col\n",
    "\n",
    "# Flatten the DataFrame by exploding array columns\n",
    "flattened_df = json_df.selectExpr(\"*\", \"explode(results) AS exploded_results\")\n",
    "\n",
    "# Select only the necessary columns for CSV conversion\n",
    "csv_df = flattened_df.select(\n",
    "    \"exploded_results.adult\",\n",
    "    \"exploded_results.backdrop_path\",\n",
    "    \"exploded_results.id\",\n",
    "    \"exploded_results.media_type\",\n",
    "    \"exploded_results.name\",\n",
    "    \"exploded_results.original_language\",\n",
    "    \"exploded_results.original_name\",\n",
    "    \"exploded_results.first_air_date\",\n",
    "    \"exploded_results.original_title\",\n",
    "    \"exploded_results.overview\",\n",
    "    \"exploded_results.popularity\",\n",
    "    \"exploded_results.poster_path\",\n",
    "    \"exploded_results.release_date\",\n",
    "    \"exploded_results.title\",\n",
    "    \"exploded_results.video\",\n",
    "    \"exploded_results.vote_average\",\n",
    "    \"exploded_results.vote_count\",\n",
    "    col(\"exploded_results.origin_country\").cast(\"string\").alias(\"origin_country_str\")\n",
    ")\n",
    "\n",
    "# Convert array column to a string representation\n",
    "csv_df = csv_df.withColumn(\"origin_country_str\", col(\"origin_country_str\").cast(\"string\"))\n",
    "csv_df = csv_df.withColumn(\"origin_country\", col(\"origin_country_str\"))\n",
    "\n",
    "# Drop the temporary string column\n",
    "csv_df = csv_df.drop(\"origin_country_str\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44adfb4c-8252-401f-a3a6-f10a6f2abc55",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, coalesce\n",
    "\n",
    "# Combine the two columns into a new column and ignore null values\n",
    "csv_df = csv_df.withColumn('Name', coalesce(col('title'), col('name')))\n",
    "csv_df = csv_df.withColumn('Release_Dates', coalesce(col('first_air_date'), col('release_date')))\n",
    "\n",
    "# Drop the original two columns if needed\n",
    "csv_df = csv_df.drop('first_air_date', 'release_date','original_name','origin_country','original_title','title','video','backdrop_path')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "963e2203-f4ab-4f95-b00c-540bdd12d1b6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import concat, lit, trim\n",
    "\n",
    "# Function to add the prefix to each poster_path value\n",
    "def add_prefix(path):\n",
    "    return 'https://image.tmdb.org/t/p/w500/' + path.strip()\n",
    "\n",
    "# Use Spark built-in functions to add the prefix to the 'poster_path' column\n",
    "csv_df = csv_df.withColumn('poster_path', concat(lit('https://image.tmdb.org/t/p/w500/'), trim(csv_df['poster_path'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3cc4084-4915-4e63-8e06-7d48a6790fb7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the desired order of columns (replace with your actual column names)\n",
    "\n",
    "csv_df = csv_df.select(\n",
    "    col(\"id\").alias(\"ID\"),\n",
    "    col(\"Name\").alias(\"Names\"),\n",
    "    col(\"Release_Dates\").alias(\"Release_Dates\"),\n",
    "    col(\"media_type\").alias(\"Media_Type\"),\n",
    "    col(\"adult\").alias(\"Adult\"),\n",
    "    col(\"original_language\").alias(\"Original_Language\"),\n",
    "    col(\"overview\").alias(\"Overview\"),\n",
    "    col(\"popularity\").alias(\"Popularity\"),\n",
    "    col(\"vote_average\").alias(\"Vote_Average\"),\n",
    "    col(\"vote_count\").alias(\"Vote_Count\"),\n",
    "    col(\"poster_path\").alias(\"Poster_Path\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa5c13aa-31da-411b-91df-b84cb3cb2d77",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#setting variables for sql server \n",
    "jdbcHostname = \"<sqlsrv-name>.database.windows.net\"\n",
    "jdbcPort = \"1433\"\n",
    "jdbcDatabase = \"<db-name>\"\n",
    "properties = {\n",
    " \"user\" : \"<username>\",\n",
    " \"password\" : \"<password>\" }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0351509-5a77-4be1-9770-7f824a67c229",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    " #jdbc url\n",
    " url = \"jdbc:sqlserver://{0}:{1};database={2}\".format(jdbcHostname,jdbcPort,jdbcDatabase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5c90976-bcef-4773-aad9-90a3ab45c40d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.databricks.v1+bamboolib_hint": "{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}",
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#connecting to sql server and define the table name.\n",
    "from pyspark.sql import *\n",
    "import pandas as pd\n",
    "myfinaldf = DataFrameWriter(csv_df)\n",
    "myfinaldf.jdbc(url=url, table= \"Movies\", mode =\"overwrite\", properties = properties)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1556279865619152,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "API to SQL",
   "widgets": {
    "utc_time": {
     "currentValue": "",
     "nuid": "4a485aa7-1f7f-42e8-8bbe-f1cb87b1aad5",
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "utc_time",
      "options": {
       "widgetType": "text",
       "validationRegex": null
      }
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
